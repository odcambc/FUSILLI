# QC Metrics Interpretation Guide

This guide explains how to interpret the quality control (QC) metrics generated by FUSILLI and how to use the MultiQC report to assess data quality and pipeline performance.

## Overview

FUSILLI generates comprehensive QC metrics that are aggregated into a single MultiQC HTML report. The metrics fall into several categories:

1. **Standard Tool Metrics**: From FastQC, BBDuk, and BBMerge
2. **Detection Performance Metrics**: How effectively fusions are detected
3. **Library Representation Metrics**: How well the library is represented
4. **Library Diversity Metrics**: Complexity and evenness of the library
5. **Preprocessing Efficiency Metrics**: Read retention through preprocessing steps
6. **Partner Detection Metrics**: Partner domain detection across samples

## Accessing the MultiQC Report

The MultiQC report is generated at:
```
stats/{experiment}/{experiment}_multiqc.html
```

Open this file in a web browser to view interactive visualizations and summary tables. The report includes:

- **General Stats Table**: Key metrics across all samples at a glance
- **Module Sections**: Detailed visualizations for each tool and metric category
- **Interactive Plots**: Hover for details, zoom, and export options

## Standard Tool Metrics

### FastQC Metrics

FastQC provides comprehensive read quality assessment.

#### Key Metrics to Monitor

- **Mean Quality Score**: Should be ≥ Q30 for high-quality data
  - **Good**: Q30-Q40
  - **Warning**: Q20-Q30
  - **Poor**: < Q20

- **%GC Content**: Should be consistent across samples and match expected library composition
  - Large deviations may indicate contamination or sequencing issues

- **Adapter Content**: Should be low (< 5%) after trimming
  - High adapter content indicates incomplete trimming

- **Sequence Duplication Levels**: Indicates library complexity
  - Low duplication (< 20%) is good
  - High duplication (> 50%) suggests low library diversity or over-amplification

- **Per-base Quality**: Quality scores should remain high across the entire read length
  - Quality degradation at read ends is normal but should be minimal

#### Interpreting FastQC Flags

FastQC assigns "pass", "warn", or "fail" flags to each metric:
- **Pass**: Metric is within acceptable range
- **Warn**: Metric is outside ideal range but may be acceptable
- **Fail**: Metric indicates a potential problem

**Note**: Some FastQC warnings (e.g., "Per base sequence content" for amplicon sequencing) are expected and not necessarily problematic.

### BBDuk Metrics

BBDuk performs adapter trimming, contaminant removal, and quality filtering.

#### Trim Step Metrics

- **Read Retention Rate**: `output_reads / input_reads`
  - **Good**: > 0.8 (80% retention)
  - **Warning**: 0.6-0.8
  - **Poor**: < 0.6

- **Base Retention Rate**: `output_bases / input_bases`
  - Typically lower than read retention due to adapter trimming
  - **Good**: > 0.7

#### Contaminant Removal Metrics

- **Contaminant Fraction**: `(input_reads - output_reads) / input_reads`
  - **Good**: < 0.05 (5% contamination)
  - **Warning**: 0.05-0.15
  - **Poor**: > 0.15

#### Quality Filtering Metrics

- **Quality Filter Retention**: `output_reads / input_reads`
  - Depends on quality threshold (typically Q30)
  - **Good**: > 0.7
  - **Warning**: 0.5-0.7
  - **Poor**: < 0.5

### BBMerge Metrics

BBMerge merges overlapping paired-end reads.

#### Key Metrics

- **Merge Rate**: `merged_reads / total_pairs`
  - **Good**: > 0.7 (70% of pairs merge)
  - **Warning**: 0.5-0.7
  - **Poor**: < 0.5

- **Average Insert Size**: Median insert size from merged reads
  - Should match expected library insert size
  - Large deviations may indicate library construction issues

- **Unmerged Fraction**: `unmerged_pairs / total_pairs`
  - Inverse of merge rate
  - High unmerged fraction may indicate:
    - Insert size too large for read length
    - Poor read quality preventing overlap detection
    - Library construction issues

## FUSILLI-Specific Metrics

### Detection Performance Metrics

These metrics assess how effectively the pipeline detects fusions in the sequencing data.

#### Detection Efficiency

- **Detection Efficiency**: `matched_reads / reads_processed`
  - Fraction of processed reads that contain detected fusion breakpoints
  - **Good**: > 0.1 (10% of reads contain fusions)
  - **Warning**: 0.05-0.1
  - **Poor**: < 0.05

- **Prefilter Efficiency**: `reads_with_partner_end / reads_processed`
  - Fraction of reads passing the partner end pre-filter
  - **Good**: > 0.3
  - **Warning**: 0.1-0.3
  - **Poor**: < 0.1

- **Matching Efficiency**: `reads_matched / reads_with_partner_end`
  - Fraction of pre-filtered reads that match breakpoint sequences
  - **Good**: > 0.3
  - **Warning**: 0.1-0.3
  - **Poor**: < 0.1

#### Detection Yield

- **Detections per Read**: `total_fusion_counts / reads_processed`
  - Average number of fusion detections per processed read
  - **Good**: > 0.1
  - **Warning**: 0.05-0.1
  - **Poor**: < 0.05

- **Detections per Million Reads**: `(total_fusion_counts / reads_processed) * 1,000,000`
  - Normalized detection rate for cross-sample comparison
  - Useful for comparing samples with different sequencing depths

#### Sensitivity Metrics

- **Sensitivity Index**: `reads_matched / expected_detectable_reads`
  - Ratio of actual detections to expected detections based on read length
  - **Good**: > 0.5 (detecting at least 50% of expected fusions)
  - **Warning**: 0.3-0.5
  - **Poor**: < 0.3

- **Expected Detection Fraction**: Fraction of reads long enough to contain breakpoint k-mers
  - Depends on read length and `breakpoint_window` setting
  - Should be > 0.8 for typical read lengths (≥ 150 bp)

### Library Representation Metrics

These metrics assess how well the fusion library is represented in the sequencing data.

#### Coverage Metrics

- **Variant Coverage**: `observed_variants / expected_variants`
  - Fraction of expected fusion variants detected
  - **Good**: > 0.8 (80% of variants detected)
  - **Warning**: 0.6-0.8
  - **Poor**: < 0.6

- **Breakpoint Coverage**: `detected_breakpoints / expected_breakpoints`
  - Fraction of expected breakpoint positions detected
  - **Good**: > 0.7
  - **Warning**: 0.5-0.7
  - **Poor**: < 0.5

- **Partner Coverage**: `detected_partners / expected_partners`
  - Fraction of expected partner domains detected
  - **Good**: > 0.9 (all or nearly all partners detected)
  - **Warning**: 0.7-0.9
  - **Poor**: < 0.7

#### Zero Fraction Metrics

- **Zero Fraction**: `variants_with_zero_counts / total_variants`
  - Fraction of variants with no detections
  - Lower is better
  - **Good**: < 0.2 (80% of variants have at least one detection)
  - **Warning**: 0.2-0.4
  - **Poor**: > 0.4

### Library Diversity Metrics

These metrics quantify the complexity and evenness of the fusion library.

#### Diversity Indices

- **Shannon Diversity Index**: `-Σ(p_i * ln(p_i))`
  - Measures both richness (number of variants) and evenness (distribution)
  - Higher values indicate greater diversity
  - **Good**: > 5.0
  - **Warning**: 3.0-5.0
  - **Poor**: < 3.0

- **Simpson Diversity Index**: `1 - Σ(p_i²)`
  - Measures probability that two randomly selected reads are different variants
  - Range: 0 (no diversity) to 1 (maximum diversity)
  - **Good**: > 0.9
  - **Warning**: 0.7-0.9
  - **Poor**: < 0.7

- **Evenness**: `shannon_diversity / ln(observed_variants)`
  - Measures how evenly counts are distributed across variants
  - Range: 0 (uneven) to 1 (perfectly even)
  - **Good**: > 0.7
  - **Warning**: 0.5-0.7
  - **Poor**: < 0.5

#### Top N Fractions

- **Top 1 Fraction**: Fraction of counts in the most abundant variant
  - Lower is better (indicates less dominance by single variant)
  - **Good**: < 0.1 (top variant < 10% of counts)
  - **Warning**: 0.1-0.3
  - **Poor**: > 0.3

- **Top 10 Fraction**: Fraction of counts in the top 10 variants
  - **Good**: < 0.5 (top 10 variants < 50% of counts)
  - **Warning**: 0.5-0.7
  - **Poor**: > 0.7

### Preprocessing Efficiency Metrics

These metrics track read retention through the preprocessing pipeline.

#### End-to-End Retention

- **Raw to Merged Read Retention**: `merged_reads / raw_reads`
  - Overall read retention from raw data to merged reads
  - **Good**: > 0.5 (50% retention)
  - **Warning**: 0.3-0.5
  - **Poor**: < 0.3

- **Total Read Loss Fraction**: `(raw_reads - merged_reads) / raw_reads`
  - Inverse of retention
  - **Good**: < 0.5
  - **Warning**: 0.5-0.7
  - **Poor**: > 0.7

#### Step-by-Step Loss Breakdown

The `decay_metrics.csv` file tracks read counts at each preprocessing step:

1. **Raw**: Initial read count
2. **Trimmed**: After adapter trimming
3. **Cleaned**: After contaminant removal
4. **Quality**: After quality filtering
5. **Merged**: After read merging

**Expected Loss Patterns**:
- **Trim step**: 5-15% loss (adapter removal)
- **Contaminant step**: 1-5% loss (PhiX, adapters)
- **Quality step**: 10-30% loss (depends on quality threshold)
- **Merge step**: 20-30% loss (unmerged pairs)

**Warning Signs**:
- Excessive loss at trim step (> 30%): May indicate adapter contamination
- Excessive loss at contaminant step (> 10%): May indicate high contamination
- Excessive loss at quality step (> 50%): May indicate poor read quality
- Low merge rate (< 50%): May indicate insert size issues or poor read quality

### Partner Detection Metrics

These metrics assess partner domain detection across samples.

#### Partner Counts

- **Partner End Counts**: Reads containing partner 3' end sequences
  - Indicates partner domain presence in reads
  - Should correlate with fusion counts for that partner

- **Partner Linker Counts**: Reads containing partner-linker junction sequences
  - More specific indicator of fusion presence
  - Should be lower than partner end counts (linker is more specific)

#### Partner Coverage

- **Partners Detected**: Number of unique partners with at least one detection
  - Should match expected number of partners in library design
  - Missing partners may indicate:
    - Library construction issues
    - Low sequencing depth
    - Partner sequences not in reference

## Using the MultiQC Report

### General Stats Table

The General Stats table at the top of the MultiQC report provides a quick overview of key metrics across all samples. Use this to:

1. **Identify Problematic Samples**: Look for samples with metrics outside expected ranges
2. **Compare Samples**: Compare metrics across conditions, replicates, or timepoints
3. **Track Trends**: Identify systematic issues affecting multiple samples

### Module Sections

Each module section provides detailed visualizations:

1. **FastQC**: Per-sample quality plots, cross-sample comparisons
2. **BBDuk**: Retention rates, histogram visualizations
3. **BBMerge**: Merge rates, insert size distributions
4. **FUSILLI Detection**: Detection efficiency plots, coverage plots, sensitivity analysis
5. **FUSILLI Diversity**: Diversity index comparisons, evenness plots, top N fractions
6. **FUSILLI Preprocessing**: Read decay plots, retention rate comparisons
7. **FUSILLI Partners**: Partner detection heatmaps, coverage plots

### Interactive Features

- **Hover for Details**: Hover over data points to see exact values
- **Zoom**: Click and drag to zoom into plot regions
- **Export**: Download plots as PNG or SVG
- **Filter**: Filter samples by condition or other metadata (if configured)

## Troubleshooting Based on Metrics

### Low Detection Efficiency (< 5%)

**Possible Causes**:
- Low library representation in sequencing data
- Poor read quality preventing breakpoint detection
- Incorrect breakpoint sequences or reference
- Read length too short for breakpoint k-mers

**Actions**:
1. Check read quality (FastQC metrics)
2. Verify breakpoint sequences match library design
3. Check read length distribution (should be ≥ 2 × `breakpoint_window`)
4. Review library construction and sequencing depth

### Low Coverage (< 60%)

**Possible Causes**:
- Insufficient sequencing depth
- Library construction issues (missing variants)
- Detection sensitivity too low
- Breakpoint sequences not matching actual library

**Actions**:
1. Check sequencing depth (total reads processed)
2. Verify library design matches expected variants
3. Review detection efficiency metrics
4. Check for systematic biases (e.g., certain partners missing)

### Low Diversity (< 3.0 Shannon)

**Possible Causes**:
- Library dominated by few variants
- Over-amplification of specific variants
- Insufficient sequencing depth
- Biased library construction

**Actions**:
1. Check top N fractions (high top 1 fraction indicates dominance)
2. Review library construction protocols
3. Check for PCR bias or over-amplification
4. Consider increasing sequencing depth

### High Read Loss (> 70%)

**Possible Causes**:
- Poor read quality
- High adapter contamination
- High contaminant levels
- Insert size issues preventing merging

**Actions**:
1. Check FastQC quality metrics
2. Review BBDuk retention at each step
3. Check merge rate (BBMerge metrics)
4. Review adapter trimming and contaminant removal settings

### Low Merge Rate (< 50%)

**Possible Causes**:
- Insert size too large for read length
- Poor read quality preventing overlap detection
- Library construction issues

**Actions**:
1. Check insert size distribution (BBMerge ihist)
2. Review read quality (FastQC)
3. Verify library insert size matches sequencing read length
4. Consider adjusting merge parameters

## Quality Thresholds Summary

| Metric | Good | Warning | Poor |
|--------|------|---------|------|
| Mean Quality (Q) | > 30 | 20-30 | < 20 |
| Merge Rate | > 0.7 | 0.5-0.7 | < 0.5 |
| Detection Efficiency | > 0.1 | 0.05-0.1 | < 0.05 |
| Variant Coverage | > 0.8 | 0.6-0.8 | < 0.6 |
| Breakpoint Coverage | > 0.7 | 0.5-0.7 | < 0.5 |
| Partner Coverage | > 0.9 | 0.7-0.9 | < 0.7 |
| Shannon Diversity | > 5.0 | 3.0-5.0 | < 3.0 |
| Simpson Diversity | > 0.9 | 0.7-0.9 | < 0.7 |
| Evenness | > 0.7 | 0.5-0.7 | < 0.7 |
| Read Retention | > 0.5 | 0.3-0.5 | < 0.3 |
| Sensitivity Index | > 0.5 | 0.3-0.5 | < 0.3 |

## Best Practices

1. **Review MultiQC Report First**: Start with the General Stats table for a quick overview
2. **Compare Across Samples**: Look for outliers or systematic issues
3. **Check Standard Tools First**: FastQC, BBDuk, and BBMerge metrics indicate data quality
4. **Interpret FUSILLI Metrics in Context**: Consider sequencing depth, library design, and experimental conditions
5. **Document Issues**: Note any metrics outside expected ranges and investigate root causes
6. **Use Thresholds as Guidelines**: Thresholds are approximate; consider your specific experimental context

## Additional Resources

- [MultiQC Documentation](https://multiqc.info/docs/)
- [FastQC Documentation](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
- [BBTools Documentation](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)

For questions or issues with QC metrics, please open a GitHub issue with:
1. Your MultiQC report (or relevant sections)
2. Key metrics values
3. Description of the issue or concern
